# InfiR2
[English](./README.md)

<p align="center">
  <b>InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</b>
</p>

<p align="center">
  <a href="https://scholar.google.com/citations?hl=zh-CN&user=1LA3TSAAAAAJ">Wenjun Wang*</a>,
  <a href="https://scholar.google.com/citations?user=VRlUiqQAAAAJ">Shuo Cai*</a>,
  <a href="https://scholar.google.com/citations?user=I6SAtGMAAAAJ&hl=en">Congkai Xie</a>, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Hongxia Yang <br>

</p>


<p align="center">
Â  <a href="https://arxiv.org/abs/2509.22536">ğŸ“„ Paper</a> &nbsp; | &nbsp;
Â  <a href="https://huggingface.co/datasets/ZaynZhu/Paper2Video">ğŸ¤— Huggingface </a> &nbsp; | &nbsp;
Â  <a href="https://infix-ai.com/research/infir2/">ğŸŒ Project Website</a> &nbsp; | &nbsp;
</p>


## ğŸ”¥ æ›´æ–°

* [x] [2025.10.8] æˆ‘ä»¬å‘å¸ƒäº† [ä»£ç ](https://github.com/InfiXAI/InfiR2) å’Œ [æ¨¡å‹](https://huggingface.co/collections/InfiX-ai/infir2-68edca7ae3c3f052b2db0eed)ã€‚
* [x] [2025.9.26] æˆ‘ä»¬å‘å¸ƒäº† [arxiv è®ºæ–‡](https://arxiv.org/abs/2509.22536)ã€‚

### ç›®å½•
- [ğŸŒŸ æ¦‚è¿°](#-æ¦‚è¿°)
- [ğŸš€ å‡†å¤‡å·¥ä½œ](#-å‡†å¤‡å·¥ä½œ)
- [ğŸ¤– FP8 æŒç»­é¢„è®­ç»ƒ](#-FP8-æŒç»­é¢„è®­ç»ƒ)
- [ğŸŒˆ FP8 ç›‘ç£å¾®è°ƒ](#-FP8-ç›‘ç£å¾®è°ƒ)
- [ğŸ“Š è¯„æµ‹](#-è¯„æµ‹)
- [ğŸ™ è‡´è°¢](#-è‡´è°¢)
- [ğŸ“Œ å¼•ç”¨](#-å¼•ç”¨)


## ğŸŒŸ æ¦‚è¿°


æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„ FP8 è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ— ç¼é›†æˆäº†æŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§ç»†ç²’åº¦çš„æ··åˆç²’åº¦é‡åŒ–ç­–ç•¥ï¼Œä»¥åœ¨ä¿æŒæ•°å€¼ä¿çœŸåº¦çš„åŒæ—¶æœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬åœ¨ 160B token è¯­æ–™åº“ä¸Šå¯¹æ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ¡ˆä¸ä»…éå¸¸ç¨³å®šï¼Œè€Œä¸”åŸºæœ¬ä¸Šæ˜¯æ— æŸçš„ï¼Œåœ¨ä¸€ç³»åˆ—æ¨ç†åŸºå‡†ä¸Šå®ç°äº†ä¸ BF16 åŸºçº¿ç›¸å½“çš„æ€§èƒ½ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™æ˜¯åœ¨æ˜¾è‘—æé«˜æ•ˆç‡çš„æƒ…å†µä¸‹å®ç°çš„ï¼ŒåŒ…æ‹¬é«˜è¾¾ 22% çš„è®­ç»ƒæ—¶é—´å‡å°‘ã€14% çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡é™ä½å’Œ 19% çš„ååé‡æå‡ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®äº† FP8 èƒ½å¤Ÿä½œä¸º BF16 çš„ä¸€ä¸ªå®ç”¨ä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæˆ‘ä»¬å‘å¸ƒäº†å®Œæ•´çš„é…å¥—ä»£ç ä»¥è¿›ä¸€æ­¥æ™®åŠå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒã€‚

<div align="center">
  <img src="assets/fp8_recipe.png" alt="æˆ‘ä»¬çš„æ–¹æ³•" width="90%">
</div>

---

- **æ˜¾å­˜ä¼˜åŒ–ä¸åŠ é€Ÿ.** ä¸å¹¿æ³›ä½¿ç”¨çš„ BF16 ç²¾åº¦ç›¸æ¯”ï¼ŒFP8 èƒ½å¤Ÿæä¾›:
  - ç«¯åˆ°ç«¯è®­ç»ƒé€Ÿåº¦æå‡é«˜è¾¾ 22%ã€‚
  - å³°å€¼å†…å­˜ä½¿ç”¨é‡èŠ‚çœé«˜è¾¾ 14%ã€‚
  - ç«¯åˆ°ç«¯ååé‡æå‡é«˜è¾¾ 19%ã€‚

  Model Size = 1.5B


  <div align="center">

  **Context Length = 32k, TP = 2, CP = 1, MBS = 1**
  |      | Forward | Backward | Total | Ratio | Peak Memory | Ratio | Throughput | Ratio |
  | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
  | BF16 | 841 ms | 2329 ms | 3170 ms | - | 57.8 GB | - | 345 TFlops | - |
  | FP8  | 875 ms | 2075 ms | 2950 ms | 0.93Ã— | 51.7 GB | 0.89Ã— | 360 TFlops | 1.04Ã— |

  **Context Length = 8k, TP = 1, CP = 1, MBS = 2**
  |      | Forward | Backward | Total | Ratio | Peak Memory | Ratio | Throughput | Ratio |
  | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
  | BF16 | 463 ms | 1567 ms | 2030 ms | - | 68.1 GB | - | 340 TFlops | - |
  | FP8  | 529 ms | 1061 ms | 1590 ms | 0.78Ã— | 58.3 GB | 0.86Ã— | 376 TFlops | 1.10Ã— |

  </div>


  Model Size = 7B

<div align="center">

**Context Length = 32k, TP = 4, CP = 1, MBS = 1**
|      | Forward | Backward | Total | Ratio | Peak Memory | Ratio | Throughput | Ratio |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| BF16 | 2790 ms | 6800 ms | 9590 ms | - | 78.1 GB | - | 409 TFlops | - |
| FP8  | 2660 ms | 5700 ms | 8360 ms | 0.87Ã— | 67.4 GB | 0.86Ã— | 461 TFlops | 1.14Ã— |

**Context Length = 32k, TP = 2, CP = 1, MBS = 1**
|      | Forward | Backward | Total | Ratio | Peak Memory | Ratio | Throughput | Ratio |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| BF16 | 1760 ms | 5320 ms | 7080 ms | - | 53.2 GB | - | 453 TFlops | - |
| FP8  | 2300 ms | 3230 ms | 5530 ms | 0.78Ã— | 50.8 GB | 0.95Ã— | 537 TFlops | 1.19Ã— |
	
</div>


## ğŸš€ ç¯å¢ƒå‡†å¤‡

ä¸ºäº†æ‹‰å–æ­¤ä»“åº“ï¼Œè¯·ä½¿ç”¨ï¼š
```bash
git clone --recursive https://github.com/InfiXAI/InfiR2
```

### ç¯å¢ƒè®¾ç½®

æˆ‘ä»¬æ”¯æŒé€šè¿‡ **Docker** è¿›è¡Œç¯å¢ƒé…ç½®ï¼Œå¹¶æä¾›**è‡ªå®šä¹‰ Docker æ–‡ä»¶**ã€‚è¯¦æƒ…è¯·å‚ç…§ä»¥ä¸‹è¯´æ˜ï¼š

### Docker è®¾ç½®

è‡ªå®šä¹‰é…ç½®çš„ Docker é•œåƒå­˜å‚¨åœ¨ [Dockerfile](docker/Dockerfile)ã€‚ä½¿ç”¨ä»¥ä¸‹ä»£ç æ„å»º Docker é•œåƒï¼š

```bash
docker build --no-cache \
    --file docker/Dockerfile \
    --build-arg HTTP_PROXY="$http_proxy" \
    --build-arg HTTPS_PROXY="$https_proxy" \
    --build-arg NO_PROXY="localhost,127.0.0.1" \
    --build-arg SGLANG_VERSION=${SGLANG_VERSION:-latest} \
    --build-arg MEGATRON_COMMIT=${MEGATRON_COMMIT:-main} \
    -t infir2-training:latest .
```

**ä¸»è¦ç»„ä»¶ï¼š**
- **åŸºç¡€é•œåƒ**: `lmsysorg/sglang:${SGLANG_VERSION}`
- **Megatron-LM**: core_v0.14.0 åˆ†æ”¯ï¼ˆNVIDIA å®˜æ–¹ç‰ˆæœ¬ï¼‰
- **TransformerEngine**: v2.4.0ï¼ˆcommit 3cd6870ï¼‰- âš ï¸ å¿…é¡»ä½¿ç”¨æ­¤ç‰ˆæœ¬ä»¥é¿å…ç²¾åº¦/æ˜¾å­˜é—®é¢˜
- **FlashAttention**: v2.7.4.post1 + Hopper æ„å»º
- **å…¶ä»–ç»„ä»¶**: slimeã€mbridgeã€torch_memory_saverã€rayã€sglang-router ç­‰

æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒ [docker/README.md](docker/README.md)ã€‚


## ğŸ¤– FP8 æŒç»­é¢„è®­ç»ƒ

æˆ‘ä»¬æä¾›äº†ä½¿ç”¨ FP8 é‡åŒ–çš„æŒç»­é¢„è®­ç»ƒ (CPT) è„šæœ¬ã€‚æˆ‘ä»¬çš„ FP8 è®­ç»ƒæ–¹æ¡ˆä¸ BF16 åŸºçº¿ç›¸æ¯”ï¼Œå®ç°äº†**é«˜è¾¾ 22% çš„è®­ç»ƒæ—¶é—´å‡å°‘**ã€**14% çš„å³°å€¼å†…å­˜ä½¿ç”¨é‡é™ä½**ä»¥åŠ**19% çš„ååé‡æå‡**ï¼ŒåŒæ—¶åœ¨æ¨ç†åŸºå‡†ä¸Šä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒ [docs/Pretrain.md](docs/Pretrain.md)

### å¯ç”¨è„šæœ¬

æˆ‘ä»¬æ”¯æŒ 7B å’Œ 1.5B æ¨¡å‹çš„çµæ´»è®­ç»ƒé…ç½®ï¼š

- **7B æ¨¡å‹**
  - å®Œæ•´è®­ç»ƒ: [InfiR2_CPT_FP8_7B.sh](scripts/CPT/InfiR2_CPT_FP8_7B.sh) - å®Œæ•´çš„é¢„çƒ­+ç¨³å®š+è¡°å‡æµç¨‹
  - ä»…è¡°å‡: [InfiR2_CPT_FP8_7B_decay.sh](scripts/CPT/InfiR2_CPT_FP8_7B_decay.sh) - å¯é€‰çš„ç‹¬ç«‹è¡°å‡é˜¶æ®µ
- **1.5B æ¨¡å‹**
  - å®Œæ•´è®­ç»ƒ: [InfiR2_CPT_FP8_1.5B.sh](scripts/CPT/InfiR2_CPT_FP8_1.5B.sh) - å®Œæ•´çš„é¢„çƒ­+ç¨³å®š+è¡°å‡æµç¨‹
  - ä»…è¡°å‡: [InfiR2_CPT_FP8_1.5B_decay.sh](scripts/CPT/InfiR2_CPT_FP8_1.5B_decay.sh) - å¯é€‰çš„ç‹¬ç«‹è¡°å‡é˜¶æ®µ

#### è¿è¡Œ

**é€‰é¡¹ 1: å®Œæ•´è®­ç»ƒæµç¨‹ (æ¨è)**

ä¸€æ¬¡æ€§è¿è¡Œå®Œæ•´çš„é¢„çƒ­+ç¨³å®š+é€€ç«è®­ç»ƒï¼š

```bash
bash scripts/CPT/InfiR2_CPT_FP8_7B.sh
```

è¿™ä¸ªè„šæœ¬å°†è‡ªåŠ¨å®Œæˆæ‰€æœ‰ä¸‰ä¸ªè®­ç»ƒé˜¶æ®µã€‚

**é€‰é¡¹ 2: ä½¿ç”¨ç‹¬ç«‹çš„é€€ç«è„šæœ¬ (é«˜çº§)**

å¦‚æœæ‚¨æƒ³ä»ç¨³å®šé˜¶æ®µçš„æŸä¸ªç‰¹å®šæ£€æŸ¥ç‚¹è¿›å…¥é€€ç«é˜¶æ®µï¼š

```bash
# é¦–å…ˆï¼Œç¡®å®šæ‚¨åœ¨ç¨³å®šé˜¶æ®µçš„æ£€æŸ¥ç‚¹
# ç„¶åä½¿ç”¨è¯¥æ£€æŸ¥ç‚¹è¿è¡Œé€€ç«è„šæœ¬
bash scripts/CPT/InfiR2_CPT_FP8_7B_decay.sh \
    --load exp/InfiR2_CPT_FP8_7B/checkpoints/iter_0035000```
```

## ğŸŒˆ FP8 ç›‘ç£å¾®è°ƒ

æˆ‘ä»¬æä¾›äº†éµå¾ª [InfiAlign](https://arxiv.org/abs/2508.05496) è®ºæ–‡ï¼Œè¿›è¡Œ FP8 é‡åŒ–çš„ä¸¤é˜¶æ®µ SFT è®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨ Ray è¿›è¡Œåˆ†å¸ƒå¼æ‰§è¡Œï¼Œå¹¶æ”¯æŒå¤šèŠ‚ç‚¹è®­ç»ƒé…ç½®ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒ [docs/SFT.md](docs/SFT.md)ã€‚

### å¯ç”¨è„šæœ¬

æˆ‘ä»¬æ”¯æŒ 7B å’Œ 1.5B æ¨¡å‹çš„çµæ´»è®­ç»ƒé…ç½®ï¼š

- 7B SFT
  - é˜¶æ®µä¸€: [InfiR2_SFT_FP8_7B_stage1.sh](scripts/SFT/InfiR2_SFT_FP8_7B_stage1.sh).
  - é˜¶æ®µäºŒ: [InfiR2_SFT_FP8_7B_stage2.sh](scripts/SFT/InfiR2_SFT_FP8_7B_stage2.sh).
- 1.5B SFT
  - é˜¶æ®µä¸€: [InfiR2_SFT_FP8_1.5B_stage1.sh](scripts/SFT/InfiR2_SFT_FP8_1.5B_stage1.sh).
  - é˜¶æ®µäºŒ: [InfiR2_SFT_FP8_1.5B_stage2.sh](scripts/SFT/InfiR2_SFT_FP8_1.5B_stage2.sh).

#### é…ç½®

**æ•°æ®é›†:** ä¿®æ”¹ `DATA_DIR` å˜é‡ï¼Œä½¿å…¶æŒ‡å‘æ‚¨çš„è®­ç»ƒæ•°æ®ï¼š
```bash
DATA_DIR=/path/to/stage1_data
```

**æ¨¡å‹é…ç½®:**
- `HF_CHECKPOINT`: æŒ‡å‘ HuggingFace æ ¼å¼çš„æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚ Qwen2.5-7B-Instruct)
- `REF_LOAD`: æŒ‡å‘ PyTorch åˆ†å¸ƒå¼æ ¼å¼çš„åŸºç¡€æ¨¡å‹æƒé‡è·¯å¾„


```bash
HF_CHECKPOINT=/path/to/base_models_hf/qwen2.5-7B-Instruct/
REF_LOAD=/path/to/base_models_/qwen2.5-7B_torch_dist/
```
#### è¿è¡Œ
é¦–å…ˆï¼Œå¯åŠ¨ Ray é›†ç¾¤ï¼š
```bash
export MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
ray start --head --node-ip-address ${MASTER_ADDR} --num-gpus 8 --disable-usage-stats --dashboard-host=0.0.0.0 --dashboard-port=8265
```

ç„¶åï¼Œå¯åŠ¨è®­ç»ƒï¼š
```bash
bash scripts/SFT/InfiR2_SFT_FP8_7B_stage1.sh
```

## ğŸ¯ FP8 å¼ºåŒ–å­¦ä¹ 

æˆ‘ä»¬çš„ RL è®­ç»ƒæµç¨‹åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆå‹ç¼©å“åº”é•¿åº¦ï¼Œç„¶åæ‰©å±•å®ƒã€‚åœ¨å¼€å§‹ RL è®­ç»ƒä¹‹å‰ï¼Œæ‚¨éœ€è¦å°† SFT æ£€æŸ¥ç‚¹è½¬æ¢ä¸º FP8 E8M0 æ ¼å¼ï¼Œä»¥ä¾¿åœ¨ç”Ÿæˆ rollout é˜¶æ®µè¿›è¡Œé«˜æ•ˆçš„ FP8 æ¨ç†ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒ [docs/RL.md](docs/RL.md)ã€‚

### ç”¨äº RL çš„æ¨¡å‹è½¬æ¢

å®Œæˆ SFT é˜¶æ®µäºŒåï¼Œè¯·å…ˆå°†æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ ¼å¼ï¼Œç„¶åå†è½¬æ¢ä¸º FP8 E8M0 æ ¼å¼ï¼š

```bash
# æ­¥éª¤ 1: å°† PyTorch åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹è½¬æ¢ä¸º HuggingFace æ ¼å¼
PYTHONPATH=training/Megatron-LM:training/slime python tools/convert_torch_dist_to_hf.py \
    --input-dir /path/to/InfiR2_SFT_FP8_stg2 \
    --output-dir /path/to/InfiR2_SFT_FP8_stg2_hf \
    --origin-hf-dir /path/to/models/Qwen2.5-7B-Instruct

# æ­¥éª¤ 2: å°† BF16 æ ¼å¼çš„ HuggingFace æ¨¡å‹è½¬æ¢ä¸º FP8 E8M0 æ ¼å¼
python tools/bf16_cast_fp8.py \
    --input-bf16-hf-path /path/to/InfiR2_SFT_FP8_stg2_hf \
    --output-fp8-hf-path /path/to/InfiR2_SFT_FP8_stg2_hf_e8m0 \
    --force-pow-2-scale True
```

è½¬æ¢åçš„ FP8 E8M0 æ¨¡å‹å°†ç”¨äº RL çš„ rollout é˜¶æ®µçš„æ¨ç†ï¼Œä»è€Œæ˜¾è‘—æå‡ç”Ÿæˆæ•ˆç‡ã€‚

- é˜¶æ®µä¸€: [InfiR2_RL_FP8_7B_stage1_4node.sh](scripts/RL/InfiR2_RL_FP8_7B_stage1_4node.sh)ï¼Œå“åº”é•¿åº¦ä¸º 8Kã€‚
- é˜¶æ®µäºŒ: [InfiR2_RL_FP8_7B_stage2_4node.sh](scripts/RL/InfiR2_RL_FP8_7B_stage2_4node.sh)ï¼Œå“åº”é•¿åº¦ä¸º 16Kï¼Œå¹¶ä½¿ç”¨æ›´é«˜çš„æ¸©åº¦ã€‚

#### é…ç½®

**æ•°æ®é›†:** å°† `DATA_DIR` è®¾ç½®ä¸ºæ‚¨çš„ RL è®­ç»ƒæ•°æ®ï¼š
```bash
DATA_DIR=/path/to/data/dapo-math-17k.jsonl
```

**æ¨¡å‹é…ç½®:**
- `HF_CHECKPOINT`: æŒ‡å‘è½¬æ¢åçš„ FP8 E8M0 æ¨¡å‹è·¯å¾„ (ç”¨äºæ¨ç†)
- `REF_LOAD`: æŒ‡å‘ SFT é˜¶æ®µäºŒçš„ PyTorch åˆ†å¸ƒå¼æ ¼å¼æ£€æŸ¥ç‚¹è·¯å¾„

```bash
HF_CHECKPOINT=/path/to/your_model/

REF_LOAD=/path/to/your_model/
```

#### è¿è¡Œ
å¯åŠ¨ RL è®­ç»ƒçš„æ–¹å¼ä¸ SFT ç›¸åŒã€‚é¦–å…ˆå¯åŠ¨ Rayï¼Œç„¶åè¿è¡Œè„šæœ¬ã€‚

è¿™ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„ç­–ç•¥å¯ä»¥ç¡®ä¿åœ¨ä¸åŒå“åº”é•¿åº¦è¦æ±‚ä¸‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæœ€ä¼˜æ€§èƒ½ã€‚


## ğŸ“Š è¯„æµ‹

æˆ‘ä»¬ä½¿ç”¨å¼€æºçš„ [evalscope](https://github.com/modelscope/evalscope) æ¡†æ¶è¿›è¡Œæ‰€æœ‰æ¨¡å‹è¯„æµ‹ï¼Œä»¥ç¡®ä¿å¯å¤ç°æ€§ã€‚æˆ‘ä»¬çš„è¯„æµ‹å¥—ä»¶åŒ…å«äº†å››ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„è¯„æµ‹è„šæœ¬ã€‚

### ç¯å¢ƒè®¾ç½®

æˆ‘ä»¬å·²ç»éªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨æœ€æ–°ç‰ˆæœ¬çš„ evalscope ä¸Šæ­£ç¡®è¿è¡Œï¼Œå¹¶å–å¾—ä¸€è‡´çš„æ€§èƒ½ç»“æœã€‚ç„¶è€Œï¼Œä¸ºäº†ä¸¥æ ¼å¤ç°æˆ‘ä»¬åœ¨è®ºæ–‡ä¸­æŠ¥å‘Šçš„è¯„æµ‹ç»“æœï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ç‰¹å®šç‰ˆæœ¬çš„ evalscopeï¼š

**ç”¨äºå¤ç°çš„æ¨èç‰ˆæœ¬:**
- ä»“åº“: [evalscope](https://github.com/modelscope/evalscope)
- åˆ†æ”¯: `main`
- æ‹‰å–è¯·æ±‚: [Add qwen-code best practice doc #734](https://github.com/modelscope/evalscope/pull/734)

**å®‰è£…:**

è¯·éµå¾ªå®˜æ–¹æ–‡æ¡£è¿›è¡Œå®‰è£… [https://evalscope.readthedocs.io/zh-cn/latest/get_started/installation.html](https://evalscope.readthedocs.io/zh-cn/latest/get_started/installation.html)

```bash
git clone https://github.com/modelscope/evalscope.git
cd evalscope/
pip install -e .
```

### è¯„æµ‹åŸºå‡†

æˆ‘ä»¬ä¸ºå››ä¸ªå…³é”®çš„æ¨ç†åŸºå‡†æä¾›äº†è¯„æµ‹è„šæœ¬ï¼š
<div align="center">

| åŸºå‡† | è„šæœ¬ | æœ€å¤§ Tokens | æ ·æœ¬æ•° | æ¸©åº¦ç³»æ•° |
|-----------|--------|------------|---------|-------------|
| AIME 2024 | [aime24_eval.sh](scripts/eval/aime24_eval.sh) | 31,000 | 32 | 0.65 |
| AIME 2025 | [aime25_eval.sh](scripts/eval/aime25_eval.sh) | 31,000 | 32 | 0.65 |
| GPQA | [gpqa_eval.sh](scripts/eval/gpqa_eval.sh) | 26,000 | 8 | 0.65 |
| LiveCodeBench | [livecodebenchv5_eval.sh](scripts/eval/livecodebenchv5_eval.sh) | 27,000 | 8 | 0.65 |

</div>

æ¯ä¸ªè„šæœ¬éƒ½ä½¿ç”¨ slurm è¿›è¡Œä½œä¸šè°ƒåº¦ï¼Œå¹¶ä½¿ç”¨ SGLang æä¾›é«˜æ•ˆçš„æ¨ç†æœåŠ¡ã€‚è¯„æµ‹æµç¨‹åŒ…æ‹¬ï¼š

1. ä½¿ç”¨æ¨¡å‹å¯åŠ¨ä¸€ä¸ª SGLang æœåŠ¡
2. è¿è¡Œ evalscope å¹¶æŒ‡å®šç›¸åº”çš„åŸºå‡†æµ‹è¯•

### æ¨¡å‹è¡¨ç°
- 7Bæ¨¡å‹

<div align="center">

<table>
  <thead>
    <tr>
      <th align="left">Model</th>
      <th align="center">AIME 25</th>
      <th align="center">AIME 24</th>
      <th align="center">GPQA</th>
      <th align="center">LiveCodeBench v5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td align="left"><strong>Deepseek-Distill-Qwen-7B</strong></td>
      <td align="center">43.00</td>
      <td align="center">49.00</td>
      <td align="center">48.20</td>
      <td align="center">37.60</td>
    </tr>
    <tr>
      <td align="left"><strong>Qwen2.5-7B-base (w. InfiAlign)</strong></td>
      <td align="center">33.75</td>
      <td align="center">43.02</td>
      <td align="center">48.11</td>
      <td align="center">39.48</td>
    </tr>
    <tr>
      <td align="left"><strong>InfiR2-7B-Instruct-FP8</strong></td>
      <td align="center">40.62</td>
      <td align="center">55.73</td>
      <td align="center">45.33</td>
      <td align="center">40.31</td>
    </tr>
    </tr>
  </tbody>
</table>

</div>


- 1.5Bæ¨¡å‹
<div align="center">

<table>
  <thead>
    <tr>
      <th align="left">Model</th>
      <th align="center">AIME 25</th>
      <th align="center">AIME 24</th>
      <th align="center">GPQA</th>
      <th align="center">LiveCodeBench v5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td align="left"><strong>Deepseek-Distill-Qwen-1.5B</strong></td>
      <td align="center">21.35</td>
      <td align="center">26.87</td>
      <td align="center">32.26</td>
      <td align="center">18.50</td>
    </tr>
    <tr>
      <td align="left"><strong>Qwen2.5-1.5B-base (w. InfiAlign)</strong></td>
      <td align="center">14.58</td>
      <td align="center">10.52</td>
      <td align="center">28.98</td>
      <td align="center">12.99</td>
    </tr>
    <tr>
      <td align="left"><strong>InfiR2-1.5B-Instruct-FP8</strong></td>
      <td align="center">18.45</td>
      <td align="center">17.39</td>
      <td align="center">29.48</td>
      <td align="center">17.10</td>
    </tr>
  </tbody>
</table>

</div>

## ğŸ™ è‡´è°¢

æˆ‘ä»¬åœ¨æ­¤å¯¹ä»¥ä¸‹å¼€æºé¡¹ç›®è¡¨ç¤ºè¯šæŒšçš„æ„Ÿè°¢ï¼š

* **[slime](https://github.com/THUDM/slime)** - ç”¨äº RL æ‰©å±•çš„å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ GLM-4.5 å’Œ GLM-4.6 çš„è®­ç»ƒã€‚slime æ”¯æŒå‡ ä¹æ‰€æœ‰ä¸ Megatron-LM å…¼å®¹çš„æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬æ­£åœ¨ä¸ slime ç¤¾åŒºç§¯æåˆä½œï¼Œè‡´åŠ›äºå®ç°å®Œå…¨è®­æ¨ä¸€è‡´çš„ FP8 RL è®­ç»ƒã€‚
* **[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)** - NVIDIA å¼€å‘çš„å¤§è§„æ¨¡ transformer æ¨¡å‹è®­ç»ƒæ¡†æ¶ã€‚
* **[TransformerEngine](https://github.com/NVIDIA/TransformerEngine)** - ç”¨äºåœ¨ NVIDIA GPU ä¸ŠåŠ é€Ÿ transformer æ¨¡å‹çš„ FP8 ç²¾åº¦åº“ã€‚
* **[Qwen2.5](https://github.com/QwenLM/Qwen2.5-Math)** - å¯å‘æˆ‘ä»¬å·¥ä½œçš„åŸºç¡€æ¨¡å‹ã€‚


## ğŸ“Œ å¼•ç”¨


å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰ç”¨ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{wang2025infir2comprehensivefp8training,
      title={InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models}, 
      author={Wenjun Wang and Shuo Cai and Congkai Xie and Mingfa Feng and Yiming Zhang and Zhen Li and Kejing Yang and Ming Li and Jiannong Cao and Hongxia Yang},
      year={2025},
      eprint={2509.22536},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.22536}, 
}
```
