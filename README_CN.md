# InfiR2

<p align="center">
  <b>InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</b>
</p>

<p align="center">
  <a href="https://scholar.google.com/citations?hl=zh-CN&user=1LA3TSAAAAAJ">Wenjun Wang*</a>,
  <a href="https://scholar.google.com/citations?user=VRlUiqQAAAAJ">Shuo Cai*</a>,
  <a href="https://scholar.google.com/citations?user=I6SAtGMAAAAJ&hl=en">Congkai Xie</a>, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Hongxia Yang <br>

</p>


<p align="center">
Â  <a href="https://arxiv.org/abs/2509.22536">ğŸ“„ Paper</a> &nbsp; | &nbsp;
Â  <a href="https://huggingface.co/datasets/ZaynZhu/Paper2Video">ğŸ¤— Huggingface </a> &nbsp; | &nbsp;
Â  <a href="https://infix-ai.com/research/infir2/">ğŸŒ Project Website</a> &nbsp; | &nbsp;
</p>


## ğŸ”¥ æ›´æ–°

* [x] [2025.10.8] æˆ‘ä»¬å‘å¸ƒäº† [ä»£ç ](https://github.com/InfiXAI/InfiR2) å’Œ [æ¨¡å‹](https://huggingface.co/collections/InfiX-ai/infir2-68edca7ae3c3f052b2db0eed)ã€‚
* [x] [2025.9.26] æˆ‘ä»¬å‘å¸ƒäº† [arxiv è®ºæ–‡](https://arxiv.org/abs/2509.22536)ã€‚

---

### ç›®å½•

* [ğŸŒŸ æ¦‚è¿°](#-æ¦‚è¿°)
* [ğŸš€ ç¯å¢ƒå‡†å¤‡](#-ç¯å¢ƒå‡†å¤‡)
* [ğŸ¤– FP8 æŒç»­é¢„è®­ç»ƒ](#-FP8_æŒç»­é¢„è®­ç»ƒ)
* [ğŸŒˆ FP8 ç›‘ç£å¾®è°ƒ](#-FP8_ç›‘ç£å¾®è°ƒ)
* [ğŸ“Š æ¨¡å‹è¯„æµ‹](#-æ¨¡å‹è¯„æµ‹)
* [ğŸ™ è‡´è°¢](#-è‡´è°¢)
* [ğŸ“Œ å¼•ç”¨](#-å¼•ç”¨)

---

## ğŸŒŸ æ¦‚è¿°

æˆ‘ä»¬æå‡ºäº†ä¸€å¥—ç«¯åˆ°ç«¯çš„ FP8 è®­ç»ƒæ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ— ç¼è¡”æ¥æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰ä¸ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç»†ç²’åº¦ã€æ··åˆç²’åº¦çš„é‡åŒ–ç­–ç•¥ï¼Œåœ¨ä¿æŒæ•°å€¼ç²¾åº¦çš„åŒæ—¶æœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡åœ¨åŒ…å« 160B token çš„è¯­æ–™ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒå®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†è¯¥æ–¹æ¡ˆå…·æœ‰æé«˜çš„ç¨³å®šæ€§ä¸å‡ ä¹æ— æŸçš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¸ BF16 åŸºçº¿å‡ ä¹ä¸€è‡´ã€‚
æ›´é‡è¦çš„æ˜¯ï¼ŒFP8 é…æ–¹åœ¨æ•ˆç‡ä¸Šå®ç°äº†æ˜¾è‘—æå‡ï¼š**è®­ç»ƒæ—¶é—´å‡å°‘ 22%**ã€**å³°å€¼æ˜¾å­˜é™ä½ 14%**ã€**ååé‡æå‡ 19%**ã€‚
æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒFP8 æ˜¯ä¸€ç§å®ç”¨ä¸”ç¨³å¥çš„ BF16 æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†å‘å¸ƒå®Œæ•´ä»£ç ï¼Œä»¥æ¨åŠ¨å¤§æ¨¡å‹è®­ç»ƒçš„æ™®æƒ åŒ–ã€‚

<div align="center">
  <img src="assets/fp8_recipe.png" alt="Our approach" width="100%">
</div>

---

## ğŸš€ ç¯å¢ƒå‡†å¤‡

å…‹éš†æœ¬ä»“åº“ï¼š

```bash
git clone --recursive https://github.com/InfiXAI/InfiR2
```

### ç¯å¢ƒé…ç½®

æˆ‘ä»¬æ”¯æŒé€šè¿‡ **Conda** å’Œ **Docker** ä¸¤ç§æ–¹å¼è¿›è¡Œç¯å¢ƒæ­å»ºï¼ŒäºŒè€…å‡åŸºäº [THUDM/slime](https://github.com/THUDM/slime) çš„å®˜æ–¹ç¯å¢ƒé…ç½®ã€‚
è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·å‚è€ƒä»¥ä¸‹é“¾æ¥ã€‚

---

### Docker ç¯å¢ƒé…ç½®

è‡ªå®šä¹‰çš„ Docker é•œåƒä½äº [Dockerfile.te_fp8.cu129](docker/Dockerfile.te_fp8.cu129)ã€‚
ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ„å»º Dockerï¼š

```bash
docker build --no-cache \
    --file docker/Dockerfile.te_fp8.cu129 \
    --build-arg HTTP_PROXY="$http_proxy" \
    --build-arg HTTPS_PROXY="$https_proxy" \
    --build-arg NO_PROXY="localhost,127.0.0.1" \
    --build-arg SGLANG_VERSION=${SGLANG_VERSION:-v0.5.0rc0-cu129} \
    --build-arg MEGATRON_COMMIT=${MEGATRON_COMMIT:-main} \
    -t infix/te-fp8:cu129 .
```

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [docker/README.md](docker/README.md)ã€‚

---

## ğŸ¤– FP8 æŒç»­é¢„è®­ç»ƒ

æˆ‘ä»¬æä¾›äº†åŸºäº FP8 é‡åŒ–çš„æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰è„šæœ¬ã€‚
è¯¥ FP8 è®­ç»ƒæ–¹æ¡ˆç›¸è¾ƒäº BF16 åŸºçº¿ï¼Œ**è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾ 22%**ã€**å³°å€¼æ˜¾å­˜é™ä½ 14%**ã€**ååé‡æå‡ 19%**ï¼ŒåŒæ—¶ä¿æŒæ¨ç†æ€§èƒ½ä¸ä¸‹é™ã€‚æ›´å¤šè¯¦æƒ…å‚è§ [docs/Pretrain.md](docs/Pretrain.md)ã€‚

### æ”¯æŒçš„è„šæœ¬

æˆ‘ä»¬æ”¯æŒ 7B å’Œ 1.5B ä¸¤ç§æ¨¡å‹è§„æ¨¡çš„çµæ´»é…ç½®ï¼š

* **7B æ¨¡å‹**

  * å®Œæ•´è®­ç»ƒæµç¨‹ï¼š[InfiR2_CPT_FP8_7B.sh](scripts/CPT/InfiR2_CPT_FP8_7B.sh)ï¼ˆåŒ…å« warmup+stable+decay ä¸‰é˜¶æ®µï¼‰
  * å•ç‹¬è¡°å‡é˜¶æ®µï¼š[InfiR2_CPT_FP8_7B_decay.sh](scripts/CPT/InfiR2_CPT_FP8_7B_decay.sh)
* **1.5B æ¨¡å‹**

  * å®Œæ•´è®­ç»ƒæµç¨‹ï¼š[InfiR2_CPT_FP8_1.5B.sh](scripts/CPT/InfiR2_CPT_FP8_1.5B.sh)
  * å•ç‹¬è¡°å‡é˜¶æ®µï¼š[InfiR2_CPT_FP8_1.5B_decay.sh](scripts/CPT/InfiR2_CPT_FP8_1.5B_decay.sh)

#### è¿è¡Œæ–¹æ³•

**æ–¹å¼ä¸€ï¼šå®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆæ¨èï¼‰**

è¿è¡Œå®Œæ•´çš„ warmup + stable + decay ä¸‰é˜¶æ®µè®­ç»ƒï¼š

```bash
bash scripts/CPT/InfiR2_CPT_FP8_7B.sh
```

è¯¥è„šæœ¬å°†è‡ªåŠ¨å®Œæˆæ‰€æœ‰é˜¶æ®µçš„è®­ç»ƒã€‚

**æ–¹å¼äºŒï¼šä»æŒ‡å®šæ£€æŸ¥ç‚¹è¿›å…¥è¡°å‡é˜¶æ®µï¼ˆè¿›é˜¶ï¼‰**

```bash
# é¦–å…ˆæ‰¾åˆ° stable é˜¶æ®µçš„ checkpoint
# ç„¶åè¿è¡Œè¡°å‡é˜¶æ®µè„šæœ¬
bash scripts/CPT/InfiR2_CPT_FP8_7B_decay.sh \
    --load exp/InfiR2_CPT_FP8_7B/checkpoints/iter_0035000
```

---

## ğŸŒˆ FP8 ç›‘ç£å¾®è°ƒ

æˆ‘ä»¬æä¾›åŸºäº FP8 é‡åŒ–çš„ä¸¤é˜¶æ®µç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒè„šæœ¬ï¼Œéµå¾ª [InfiAlign](https://arxiv.org/abs/2508.05496) çš„æ–¹æ³•ã€‚
è¯¥è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨ Ray è¿›è¡Œåˆ†å¸ƒå¼æ‰§è¡Œï¼Œå¹¶æ”¯æŒå¤šèŠ‚ç‚¹è®­ç»ƒã€‚æ›´å¤šè¯¦æƒ…å‚è§ [docs/SFT.md](docs/SFT.md)ã€‚

### æ”¯æŒçš„è„šæœ¬

æˆ‘ä»¬æ”¯æŒ 7B å’Œ 1.5B æ¨¡å‹çš„å¤šé˜¶æ®µè®­ç»ƒé…ç½®ï¼š

* 7B æ¨¡å‹

  * ç¬¬ä¸€é˜¶æ®µï¼š[InfiR2_SFT_FP8_7B_stage1.sh](scripts/SFT/InfiR2_SFT_FP8_7B_stage1.sh)
  * ç¬¬äºŒé˜¶æ®µï¼š[InfiR2_SFT_FP8_7B_stage2.sh](scripts/SFT/InfiR2_SFT_FP8_7B_stage2.sh)
* 1.5B æ¨¡å‹

  * ç¬¬ä¸€é˜¶æ®µï¼š[InfiR2_SFT_FP8_1.5B_stage1.sh](scripts/SFT/InfiR2_SFT_FP8_1.5B_stage1.sh)
  * ç¬¬äºŒé˜¶æ®µï¼š[InfiR2_SFT_FP8_1.5B_stage2.sh](scripts/SFT/InfiR2_SFT_FP8_1.5B_stage2.sh)

#### å‚æ•°é…ç½®

**æ•°æ®é›†è·¯å¾„ï¼š**

```bash
DATA_DIR=/path/to/stage1_data
```

**æ¨¡å‹è·¯å¾„ï¼š**

```bash
HF_CHECKPOINT=/path/to/base_models_hf/qwen2.5-7B/
REF_LOAD=/path/to/base_models_/qwen2.5-7B_torch_dist/
```

#### è¿è¡Œæ–¹æ³•

é¦–å…ˆå¯åŠ¨ Ray é›†ç¾¤ï¼š

```bash
ray start --head --node-ip-address ${MASTER_ADDR} --num-gpus 8 --disable-usage-stats --dashboard-host=0.0.0.0 --dashboard-port=8265
```

ç„¶åè¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

```bash
bash scripts/SFT/InfiR2_SFT_FP8_7B_stage1.sh
```

---

## ğŸ¯ FP8 å¼ºåŒ–å­¦ä¹ é˜¶æ®µ

æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æµç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼š

1. **å‹ç¼©å›å¤é•¿åº¦é˜¶æ®µ**
2. **æ‰©å±•å›å¤é•¿åº¦é˜¶æ®µ**

åœ¨ RL è®­ç»ƒå‰ï¼Œéœ€è¦å°† SFT é˜¶æ®µ 2 çš„æ¨¡å‹è½¬æ¢ä¸º FP8 E8M0 æ ¼å¼ï¼Œä»¥ä¾¿åœ¨ rollout é˜¶æ®µè¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚
æ›´å¤šç»†èŠ‚è§ [docs/RL.md](docs/RL.md)ã€‚

### æ¨¡å‹è½¬æ¢

```bash
# ç¬¬ä¸€æ­¥ï¼šå°† PyTorch åˆ†å¸ƒå¼æƒé‡è½¬ä¸º HuggingFace æ ¼å¼
PYTHONPATH=training/Megatron-LM:training/slime python tools/convert_torch_dist_to_hf.py \
    --input-dir /path/to/InfiR2_SFT_FP8_stg2 \
    --output-dir /path/to/InfiR2_SFT_FP8_stg2_hf \
    --origin-hf-dir /path/to/models/Qwen2.5-7B

# ç¬¬äºŒæ­¥ï¼šå°† BF16 æ¨¡å‹è½¬æ¢ä¸º FP8 E8M0 æ ¼å¼
python tools/bf16_cast_fp8.py \
    --input-bf16-hf-path /path/to/InfiR2_SFT_FP8_stg2_hf \
    --output-fp8-hf-path /path/to/InfiR2_SFT_FP8_stg2_hf_e8m0 \
    --force-pow-2-scale True
```

è½¬æ¢åçš„ FP8 E8M0 æ¨¡å‹å°†åœ¨ rollout é˜¶æ®µä½¿ç”¨ï¼Œå¤§å¹…æå‡æ¨ç†æ•ˆç‡ã€‚

* é˜¶æ®µ 1ï¼š[InfiR2_RL_FP8_7B_stage1_4node.sh](scripts/RL/InfiR2_RL_FP8_7B_stage1_4node.sh)
* é˜¶æ®µ 2ï¼š[InfiR2_RL_FP8_7B_stage2_4node.sh](scripts/RL/InfiR2_RL_FP8_7B_stage2_4node.sh)

#### å‚æ•°é…ç½®

**æ•°æ®é›†è·¯å¾„ï¼š**

```bash
DATA_DIR=/path/to/data/dapo-math-17k.jsonl
```

**æ¨¡å‹è·¯å¾„ï¼š**

```bash
HF_CHECKPOINT=/path/to/InfiR2_SFT_FP8_stg2_hf_e8m0/
REF_LOAD=/path/to/InfiR2_SFT_FP8_stg2/
```

#### è¿è¡Œæ–¹æ³•

ä¸ SFT ç›¸åŒï¼Œå…ˆå¯åŠ¨ Rayï¼Œå†æ‰§è¡Œè„šæœ¬ã€‚
è¯¥è¯¾ç¨‹å¼è®­ç»ƒç­–ç•¥å¯ç¡®ä¿ç¨³å®šè®­ç»ƒï¼Œå¹¶åœ¨ä¸åŒå›å¤é•¿åº¦ä¸‹å®ç°æœ€ä¼˜æ€§èƒ½ã€‚

---

## ğŸ“Š æ¨¡å‹è¯„æµ‹

æˆ‘ä»¬åŸºäºå¼€æºæ¡†æ¶ [evalscope](https://github.com/modelscope/evalscope) è¿›è¡Œæ‰€æœ‰è¯„æµ‹ï¼Œä»¥ç¡®ä¿å¯å¤ç°æ€§ã€‚
è¯„æµ‹è¦†ç›–å››ä¸ªæ¨ç†ç±»åŸºå‡†ä»»åŠ¡ï¼Œå¹¶æä¾›é…å¥—è„šæœ¬ã€‚

### ç¯å¢ƒé…ç½®

æˆ‘ä»¬éªŒè¯äº†æ¨¡å‹ä¸æœ€æ–°ç‰ˆ evalscope çš„å…¼å®¹æ€§ã€‚
è‹¥éœ€ä¸¥æ ¼å¤ç°è®ºæ–‡ç»“æœï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ç‰¹å®šç‰ˆæœ¬ï¼š

* ä»“åº“ï¼š[evalscope](https://github.com/modelscope/evalscope)
* åˆ†æ”¯ï¼š`main`
* PRï¼š[Add qwen-code best practice doc #734](https://github.com/modelscope/evalscope/pull/734)

å®‰è£…æ–¹å¼ï¼š

```bash
git clone https://github.com/modelscope/evalscope.git
cd evalscope/
pip install -e .
```

### è¯„æµ‹åŸºå‡†

| ä»»åŠ¡            | è„šæœ¬                                                              | æœ€å¤§ Token æ•° | æ ·æœ¬æ•° | æ¸©åº¦   |
| ------------- | --------------------------------------------------------------- | ---------- | --- | ---- |
| AIME 2024     | [aime24_eval.sh](scripts/eval/aime24_eval.sh)                   | 31,000     | 32  | 0.65 |
| AIME 2025     | [aime25_eval.sh](scripts/eval/aime25_eval.sh)                   | 31,000     | 32  | 0.65 |
| GPQA          | [gpqa_eval.sh](scripts/eval/gpqa_eval.sh)                       | 26,000     | 8   | 0.65 |
| LiveCodeBench | [livecodebenchv5_eval.sh](scripts/eval/livecodebenchv5_eval.sh) | 27,000     | 8   | 0.65 |

æ¯ä¸ªè„šæœ¬å‡ä½¿ç”¨ Slurm è°ƒåº¦ä»»åŠ¡ï¼Œå¹¶ç”± SGLang æä¾›é«˜æ•ˆæ¨ç†æœåŠ¡ã€‚

---

## ğŸ™ è‡´è°¢

æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„æ”¯æŒï¼š
[Slime](https://github.com/THUDM/slime)ã€[Megatron](https://github.com/NVIDIA/Megatron-LM)ã€[TransformerEngine](https://github.com/NVIDIA/TransformerEngine)ã€[Qwen2.5](https://github.com/QwenLM/Qwen2.5-Math)ã€‚

---

## ğŸ“Œ å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æœ¬å·¥ä½œæœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š

```bibtex
@misc{wang2025infir2comprehensivefp8training,
      title={InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models}, 
      author={Wenjun Wang and Shuo Cai and Congkai Xie and Mingfa Feng and Yiming Zhang and Zhen Li and Kejing Yang and Ming Li and Jiannong Cao and Hongxia Yang},
      year={2025},
      eprint={2509.22536},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.22536}, 
}
```
