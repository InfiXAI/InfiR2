# InfiR2

<p align="center">
  <b>InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</b>
</p>

<p align="center">
  <a href="https://scholar.google.com/citations?hl=zh-CN&user=1LA3TSAAAAAJ">Wenjun Wang*</a>,
  <a href="https://scholar.google.com/citations?user=VRlUiqQAAAAJ">Shuo Cai*</a>,
  <a href="https://scholar.google.com/citations?user=I6SAtGMAAAAJ&hl=en">Congkai Xie</a>, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Hongxia Yang <br>

</p>


<p align="center">
Â  <a href="https://arxiv.org/abs/2509.22536">ğŸ“„ Paper</a> &nbsp; | &nbsp;
Â  <a href="https://huggingface.co/datasets/ZaynZhu/Paper2Video">ğŸ¤— Huggingface </a> &nbsp; | &nbsp;
Â  <a href="https://infix-ai.com/research/infir2/">ğŸŒ Project Website</a> &nbsp; | &nbsp;
</p>


## ğŸ”¥ æ›´æ–°

* [x] [2025.10.8] æˆ‘ä»¬å‘å¸ƒäº† [ä»£ç ](https://github.com/InfiXAI/InfiR2) å’Œ [æ¨¡å‹](https://huggingface.co/collections/InfiX-ai/infir2-68edca7ae3c3f052b2db0eed)ã€‚
* [x] [2025.9.26] æˆ‘ä»¬å‘å¸ƒäº† [arxiv è®ºæ–‡](https://arxiv.org/abs/2509.22536)ã€‚

---

### ç›®å½•

* [ğŸŒŸ æ¦‚è¿°](#-æ¦‚è¿°)
* [ğŸš€ ç¯å¢ƒå‡†å¤‡](#-ç¯å¢ƒå‡†å¤‡)
* [ğŸ¤– FP8 æŒç»­é¢„è®­ç»ƒ](#-FP8_æŒç»­é¢„è®­ç»ƒ)
* [ğŸŒˆ FP8 ç›‘ç£å¾®è°ƒ](#-FP8_ç›‘ç£å¾®è°ƒ)
* [ğŸ“Š æ¨¡å‹è¯„æµ‹](#-æ¨¡å‹è¯„æµ‹)
* [ğŸ™ è‡´è°¢](#-è‡´è°¢)
* [ğŸ“Œ å¼•ç”¨](#-å¼•ç”¨)

## ğŸŒŸ æ¦‚è¿°

æˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ª**ç«¯åˆ°ç«¯**çš„ $\text{FP8}$ è®­ç»ƒæ–¹æ¡ˆï¼Œæ— ç¼é›†æˆäº†æŒç»­é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§**ç»†ç²’åº¦ã€æ··åˆç²¾åº¦ç²’åº¦çš„é‡åŒ–ç­–ç•¥**ï¼Œä»¥åœ¨ä¿æŒæ•°å€¼å‡†ç¡®æ€§çš„åŒæ—¶æœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬åœ¨ $\text{1600}$ äº¿ $\text{token}$ è¯­æ–™åº“ä¸Šå¯¹æ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ¡ˆä¸ä»…**æå…¶ç¨³å®š**ï¼Œè€Œä¸”**åŸºæœ¬ä¸Šæ˜¯æ— æŸçš„**ï¼Œåœ¨ä¸€ç³»åˆ—æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¸ $\text{BF16}$ åŸºçº¿**ç›¸å½“çš„æ€§èƒ½**ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™åœ¨å®ç°æ€§èƒ½æ— æŸçš„åŒæ—¶ï¼Œè¿˜å¸¦æ¥äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼ŒåŒ…æ‹¬**è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾ 22%**ã€**å³°å€¼å†…å­˜ä½¿ç”¨å‡å°‘ 14%** å’Œ**ååé‡å¢åŠ  19%**ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº† $\text{FP8}$ ä½œä¸º $\text{BF16}$ å®ç”¨ä¸”å¼ºå¤§çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†å‘å¸ƒé…å¥—ä»£ç ä»¥è¿›ä¸€æ­¥æ¨åŠ¨å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ°‘ä¸»åŒ–ã€‚

<div align="center">
  <img src="assets/fp8_recipe.png" alt="æˆ‘ä»¬çš„æ–¹æ³•" width="100%">
</div>

---

## ğŸš€ å‡†å¤‡å·¥ä½œ

å…‹éš†æ­¤ä»“åº“ï¼Œè¯·ä½¿ç”¨ï¼š
```bash
git clone --recursive [https://github.com/InfiXAI/InfiR2](https://github.com/InfiXAI/InfiR2)
````

### ç¯å¢ƒè®¾ç½®

æˆ‘ä»¬æ”¯æŒé€šè¿‡ **Conda** å’Œ **Docker** è¿›è¡Œç¯å¢ƒè®¾ç½®ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½åŸºäº [THUDM/slime](https://github.com/THUDM/slime) ä»“åº“çš„å®˜æ–¹è®¾ç½®æŒ‡å—ã€‚è¯·éµå¾ªä»¥ä¸‹é“¾æ¥ä¸­çš„è¯´æ˜ã€‚

-----

### Docker è®¾ç½®

è‡ªå®šä¹‰é…ç½®çš„ $\text{Docker}$ é•œåƒå­˜å‚¨åœ¨ [Dockerfile.te\_fp8.cu129](https://www.google.com/search?q=docker/Dockerfile.te_fp8.cu129)ã€‚ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿è¡Œ $\text{Docker}$ï¼š

```base
docker build --no-cache \
    --file docker/Dockerfile.te_fp8.cu129 \
    --build-arg HTTP_PROXY="$http_proxy" \
    --build-arg HTTPS_PROXY="$https_proxy" \
    --build-arg NO_PROXY="localhost,127.0.0.1" \
    --build-arg SGLANG_VERSION=${SGLANG_VERSION:-v0.5.0rc0-cu129} \
    --build-arg MEGATRON_COMMIT=${MEGATRON_COMMIT:-main} \
    -t infix/te-fp8:cu129 .
```

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [docker/README.md](https://www.google.com/search?q=docker/README.md)ã€‚

-----

## ğŸ¤– FP8 æŒç»­é¢„è®­ç»ƒ

æˆ‘ä»¬æä¾›äº†ä½¿ç”¨ $\text{FP8}$ é‡åŒ–çš„æŒç»­é¢„è®­ç»ƒ ($\text{CPT}$) è„šæœ¬ã€‚æˆ‘ä»¬çš„ $\text{FP8}$ è®­ç»ƒæ–¹æ¡ˆå®ç°äº†**è®­ç»ƒæ—¶é—´å‡å°‘é«˜è¾¾ 22%**ã€**å³°å€¼å†…å­˜ä½¿ç”¨å‡å°‘ 14%** å’Œ**ååé‡å¢åŠ  19%**ï¼ŒåŒæ—¶åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ä¸ $\text{BF16}$ åŸºçº¿ç›¸å½“çš„æ€§èƒ½ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [docs/Pretrain.md](https://www.google.com/search?q=docs/Pretrain.md)ã€‚

### å¯ç”¨è„šæœ¬

æˆ‘ä»¬æ”¯æŒ $\text{7B}$ å’Œ $\text{1.5B}$ æ¨¡å‹çš„çµæ´»è®­ç»ƒé…ç½®ï¼š

  - **7B æ¨¡å‹**
      - å®Œæ•´è®­ç»ƒï¼š[InfiR2\_CPT\_FP8\_7B.sh](https://www.google.com/search?q=scripts/CPT/InfiR2_CPT_FP8_7B.sh) - å®Œæ•´çš„ $\text{warmup}$ + $\text{stable}$ + $\text{decay}$ æµç¨‹
      - ä»… $\text{Decay}$ï¼š[InfiR2\_CPT\_FP8\_7B\_decay.sh](https://www.google.com/search?q=scripts/CPT/InfiR2_CPT_FP8_7B_decay.sh) - å¯é€‰çš„ç‹¬ç«‹ $\text{decay}$ é˜¶æ®µ
  - **1.5B æ¨¡å‹**
      - å®Œæ•´è®­ç»ƒï¼š[InfiR2\_CPT\_FP8\_1.5B.sh](https://www.google.com/search?q=scripts/CPT/InfiR2_CPT_FP8_1.5B.sh) - å®Œæ•´çš„ $\text{warmup}$ + $\text{stable}$ + $\text{decay}$ æµç¨‹
      - ä»… $\text{Decay}$ï¼š[InfiR2\_CPT\_FP8\_1.5B\_decay.sh](https://www.google.com/search?q=scripts/CPT/InfiR2_CPT_FP8_1.5B_decay.sh) - å¯é€‰çš„ç‹¬ç«‹ $\text{decay}$ é˜¶æ®µ

#### è¿è¡Œ

**é€‰é¡¹ 1ï¼šå®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆæ¨èï¼‰**

ä¸€é”®è¿è¡Œå®Œæ•´çš„ $\text{warmup}$ + $\text{stable}$ + $\text{decay}$ è®­ç»ƒï¼š

```bash
bash scripts/CPT/InfiR2_CPT_FP8_7B.sh
```

æ­¤å•ä¸ªè„šæœ¬å°†è‡ªåŠ¨å®Œæˆæ‰€æœ‰ä¸‰ä¸ªè®­ç»ƒé˜¶æ®µã€‚

**é€‰é¡¹ 2ï¼šä½¿ç”¨ç‹¬ç«‹ $\text{Decay}$ è„šæœ¬ï¼ˆé«˜çº§ï¼‰**

å¦‚æœæ‚¨æƒ³ä» $\text{stable}$ é˜¶æ®µçš„ç‰¹å®šæ£€æŸ¥ç‚¹è¿›å…¥ $\text{decay}$ é˜¶æ®µï¼š

```bash
# é¦–å…ˆï¼Œç¡®å®šæ‚¨çš„ stable é˜¶æ®µæ£€æŸ¥ç‚¹
# ç„¶åè¿è¡Œ decay è„šæœ¬å¹¶æŒ‡å®šè¯¥æ£€æŸ¥ç‚¹
bash scripts/CPT/InfiR2_CPT_FP8_7B_decay.sh \
    --load exp/InfiR2_CPT_FP8_7B/checkpoints/iter_0035000
```

-----

## ğŸŒˆ FP8 ç›‘ç£å¾®è°ƒ

æˆ‘ä»¬æä¾›äº†éµå¾ª [InfiAlign](https://arxiv.org/abs/2508.05496) çš„ä¸¤é˜¶æ®µ $\text{FP8}$ é‡åŒ– $\text{SFT}$ è®­ç»ƒè„šæœ¬ã€‚è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨ $\text{Ray}$ è¿›è¡Œåˆ†å¸ƒå¼æ‰§è¡Œï¼Œå¹¶æ”¯æŒå¤šèŠ‚ç‚¹è®­ç»ƒé…ç½®ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [docs/SFT.md](https://www.google.com/search?q=docs/SFT.md)ã€‚

### å¯ç”¨è„šæœ¬

æˆ‘ä»¬æ”¯æŒ $\text{7B}$ å’Œ $\text{1.5B}$ æ¨¡å‹çš„çµæ´»è®­ç»ƒé…ç½®ï¼š

  - 7B $\text{SFT}$
      - é˜¶æ®µ 1ï¼š[InfiR2\_SFT\_FP8\_7B\_stage1.sh](https://www.google.com/search?q=scripts/SFT/InfiR2_SFT_FP8_7B_stage1.sh)ã€‚
      - é˜¶æ®µ 2ï¼š[InfiR2\_SFT\_FP8\_7B\_stage2.sh](https://www.google.com/search?q=scripts/SFT/InfiR2_SFT_FP8_7B_stage2.sh)ã€‚
  - 1.5B $\text{SFT}$
      - é˜¶æ®µ 1ï¼š[InfiR2\_SFT\_FP8\_1.5B\_stage1.sh](https://www.google.com/search?q=scripts/SFT/InfiR2_SFT_FP8_1.5B_stage1.sh)ã€‚
      - é˜¶æ®µ 2ï¼š[InfiR2\_SFT\_FP8\_1.5B\_stage2.sh](https://www.google.com/search?q=scripts/SFT/InfiR2_SFT_FP8_1.5B_stage2.sh)ã€‚

#### é…ç½®

**æ•°æ®é›†ï¼š** ä¿®æ”¹ $\text{DATA\_DIR}$ å˜é‡ä»¥æŒ‡å‘æ‚¨çš„è®­ç»ƒæ•°æ®ï¼š

```bash
DATA_DIR=/path/to/stage1_data
```

**æ¨¡å‹é…ç½®ï¼š**

  - `HF_CHECKPOINT`ï¼š$\text{HuggingFace}$ æ ¼å¼çš„åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä¾‹å¦‚ $\text{Qwen2.5-7B}$ï¼‰
  - `REF_LOAD`ï¼š$\text{PyTorch Distributed}$ æ ¼å¼çš„åŸºç¡€æ¨¡å‹æƒé‡è·¯å¾„

<!-- end list -->

```bash
HF_CHECKPOINT=/path/to/base_models_hf/qwen2.5-7B/
REF_LOAD=/path/to/base_models_/qwen2.5-7B_torch_dist/
```

#### è¿è¡Œ

é¦–å…ˆï¼Œå¯åŠ¨ $\text{Ray}$ é›†ç¾¤ï¼š

```bash
ray start --head --node-ip-address ${MASTER_ADDR} --num-gpus 8 --disable-usage-stats --dashboard-host=0.0.0.0 --dashboard-port=8265
```

ç„¶åå¯åŠ¨è®­ç»ƒï¼š

```bash
bash scripts/SFT/InfiR2_SFT_FP8_7B_stage1.sh
```

-----

## ğŸ¯ FP8 å¼ºåŒ–å­¦ä¹ 

æˆ‘ä»¬çš„ $\text{RL}$ è®­ç»ƒæµç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆå‹ç¼©å“åº”é•¿åº¦ï¼Œç„¶åæ‰©å±•å“åº”é•¿åº¦ã€‚åœ¨ $\text{RL}$ è®­ç»ƒä¹‹å‰ï¼Œæ‚¨éœ€è¦å°† $\text{SFT}$ æ£€æŸ¥ç‚¹è½¬æ¢ä¸º $\text{FP8 E8M0}$ æ ¼å¼ï¼Œä»¥æé«˜ $\text{rollout}$ ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ $\text{FP8}$ æ¨ç†æ•ˆç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [docs/RL.md](https://www.google.com/search?q=docs/RL.md)ã€‚

### $\text{RL}$ æ¨¡å‹è½¬æ¢

å®Œæˆ $\text{SFT}$ é˜¶æ®µ $\text{2}$ åï¼Œå°†æ¨¡å‹è½¬æ¢ä¸º $\text{HuggingFace}$ æ ¼å¼ï¼Œç„¶åå†è½¬æ¢ä¸º $\text{FP8 E8M0}$ æ ¼å¼ï¼š

```bash
# æ­¥éª¤ 1: å°† PyTorch distributed æ£€æŸ¥ç‚¹è½¬æ¢ä¸º HuggingFace æ ¼å¼
PYTHONPATH=training/Megatron-LM:training/slime python tools/convert_torch_dist_to_hf.py \
    --input-dir /path/to/InfiR2_SFT_FP8_stg2 \
    --output-dir /path/to/InfiR2_SFT_FP8_stg2_hf \
    --origin-hf-dir /path/to/models/Qwen2.5-7B

# æ­¥éª¤ 2: å°† BF16 HuggingFace æ¨¡å‹è½¬æ¢ä¸º FP8 E8M0 æ ¼å¼
python tools/bf16_cast_fp8.py \
    --input-bf16-hf-path /path/to/InfiR2_SFT_FP8_stg2_hf \
    --output-fp8-hf-path /path/to/InfiR2_SFT_FP8_stg2_hf_e8m0 \
    --force-pow-2-scale True
```

$\text{FP8 E8M0}$ æ¨¡å‹å°†ç”¨äº $\text{RL rollout}$ é˜¶æ®µçš„æ¨ç†ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡ã€‚

  - é˜¶æ®µ 1ï¼š[InfiR2\_RL\_FP8\_7B\_stage1\_4node.sh](https://www.google.com/search?q=scripts/RL/InfiR2_RL_FP8_7B_stage1_4node.sh)ï¼Œå“åº”é•¿åº¦ä¸º $\text{8K}$ã€‚
  - é˜¶æ®µ 2ï¼š[InfiR2\_RL\_FP8\_7B\_stage2\_4node.sh](https://www.google.com/search?q=scripts/RL/InfiR2_RL_FP8_7B_stage2_4node.sh)ï¼Œå“åº”é•¿åº¦ä¸º $\text{16K}$ï¼Œæ¸©åº¦æ›´é«˜ã€‚

#### é…ç½®

**æ•°æ®é›†ï¼š** è®¾ç½® $\text{DATA\_DIR}$ ä¸ºæ‚¨çš„ $\text{RL}$ è®­ç»ƒæ•°æ®ï¼š

```bash
DATA_DIR=/path/to/data/dapo-math-17k.jsonl
```

**æ¨¡å‹é…ç½®ï¼š**

  - `HF_CHECKPOINT`ï¼šè½¬æ¢åçš„ $\text{FP8 E8M0}$ æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºæ¨ç†ï¼‰
  - `REF_LOAD`ï¼š$\text{PyTorch Distributed}$ æ ¼å¼çš„ $\text{SFT}$ é˜¶æ®µ $\text{2}$ æ£€æŸ¥ç‚¹è·¯å¾„

<!-- end list -->

```bash
HF_CHECKPOINT=/path/to/your_model/

REF_LOAD=/path/to/your_model/
```

#### è¿è¡Œ

å¯åŠ¨ $\text{RL}$ è®­ç»ƒçš„æ–¹å¼ä¸ $\text{SFT}$ ç›¸åŒã€‚é¦–å…ˆå¯åŠ¨ $\text{Ray}$ï¼Œç„¶åè¿è¡Œè„šæœ¬ã€‚

è¿™ç§åŸºäºè¯¾ç¨‹çš„ç­–ç•¥ç¡®ä¿äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶åœ¨ä¸åŒçš„å“åº”é•¿åº¦è¦æ±‚ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚

-----

## ğŸ“Š è¯„ä¼°

æˆ‘ä»¬ä½¿ç”¨å¼€æºçš„ [evalscope](https://github.com/modelscope/evalscope) æ¡†æ¶è¿›è¡Œæ‰€æœ‰æ¨¡å‹è¯„ä¼°ï¼Œä»¥ç¡®ä¿å¯å¤ç°æ€§ã€‚æˆ‘ä»¬çš„è¯„ä¼°å¥—ä»¶åŒ…æ‹¬å››ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„è¯„ä¼°è„šæœ¬ã€‚

### ç¯å¢ƒè®¾ç½®

æˆ‘ä»¬å·²éªŒè¯æ¨¡å‹åœ¨æœ€æ–°ç‰ˆæœ¬çš„ $\text{evalscope}$ ä¸‹å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œå¹¶èƒ½è¾¾åˆ°ä¸€è‡´çš„æ€§èƒ½ç»“æœã€‚ä½†æ˜¯ï¼Œä¸ºäº†ä¸¥æ ¼å¤ç°æˆ‘ä»¬è®ºæ–‡ä¸­æŠ¥å‘Šçš„å‡†ç¡®è¯„ä¼°ç»“æœï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ç‰¹å®šç‰ˆæœ¬çš„ $\text{evalscope}$ï¼š

**å»ºè®®ç”¨äºå¤ç°çš„ç‰ˆæœ¬ï¼š**

  - ä»“åº“ï¼š[evalscope](https://github.com/modelscope/evalscope)
  - åˆ†æ”¯ï¼š`main`
  - æ‹‰å–è¯·æ±‚ ($\text{Pull Request}$)ï¼š[Add qwen-code best practice doc \#734](https://github.com/modelscope/evalscope/pull/734)

**å®‰è£…ï¼š**

éµå¾ªå®˜æ–¹æ–‡æ¡£ [https://evalscope.readthedocs.io/zh-cn/latest/get\_started/installation.html](https://evalscope.readthedocs.io/zh-cn/latest/get_started/installation.html)

```bash
git clone [https://github.com/modelscope/evalscope.git](https://github.com/modelscope/evalscope.git)
cd evalscope/
pip install -e .
```

### è¯„ä¼°åŸºå‡†

æˆ‘ä»¬ä¸ºå››ä¸ªå…³é”®æ¨ç†åŸºå‡†æä¾›äº†è¯„ä¼°è„šæœ¬ï¼š

| åŸºå‡† | è„šæœ¬ | æœ€å¤§ $\text{Token}$ æ•° | æ ·æœ¬æ•° | æ¸©åº¦ |
| :---: | :---: | :---: | :---: | :---: |
| $\text{AIME 2024}$ | [aime24\_eval.sh](https://www.google.com/search?q=scripts/eval/aime24_eval.sh) | $\text{31,000}$ | 32 | 0.65 |
| $\text{AIME 2025}$ | [aime25\_eval.sh](https://www.google.com/search?q=scripts/eval/aime25_eval.sh) | $\text{31,000}$ | 32 | 0.65 |
| $\text{GPQA}$ | [gpqa\_eval.sh](https://www.google.com/search?q=scripts/eval/gpqa_eval.sh) | $\text{26,000}$ | 8 | 0.65 |
| $\text{LiveCodeBench}$ | [livecodebenchv5\_eval.sh](https://www.google.com/search?q=scripts/eval/livecodebenchv5_eval.sh) | $\text{27,000}$ | 8 | 0.65 |

### è¿è¡Œè¯„ä¼°

æ¯ä¸ªè„šæœ¬éƒ½ä½¿ç”¨ $\text{slurm}$ è¿›è¡Œä½œä¸šè°ƒåº¦ï¼Œå¹¶ä½¿ç”¨ $\text{SGLang}$ è¿›è¡Œé«˜æ•ˆæ¨ç†æœåŠ¡ã€‚è¯„ä¼°æµç¨‹åŒ…æ‹¬ï¼š

1.  ä½¿ç”¨æ¨¡å‹å¯åŠ¨ $\text{SGLang}$ æœåŠ¡å™¨
2.  ä½¿ç”¨æŒ‡å®šçš„åŸºå‡†è¿è¡Œ $\text{evalscope}$

-----

## ğŸ™ è‡´è°¢

  * æˆ‘ä»¬è¦æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š[Slime](https://github.com/THUDM/slime), [Megatron](https://github.com/NVIDIA/Megatron-LM), [TransformerEngine](https://github.com/NVIDIA/TransformerEngine) å’Œ [Qwen2.5](https://github.com/QwenLM/Qwen2.5-Math)ã€‚

-----

## ğŸ“Œ å¼•ç”¨

å¦‚æœæˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{wang2025infir2comprehensivefp8training,
      title={InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models}, 
      author={Wenjun Wang and Shuo Cai and Congkai Xie and Mingfa Feng and Yiming Zhang and Zhen Li and Kejing Yang and Ming Li and Jiannong Cao and Hongxia Yang},
      year={2025},
      eprint={2509.22536},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={[https://arxiv.org/abs/2509.22536](https://arxiv.org/abs/2509.22536)}, 
}
```